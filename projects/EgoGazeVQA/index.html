<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EgoGazeVQA</title>
    
    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    
    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <!-- Custom CSS -->
    <link rel="stylesheet" href="assets/css/style.css">
    
    <!-- Prism.js for code highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />

    <!-- Simple Icons for Hugging Face -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/simple-icons@latest/icons/huggingface.svg">

</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-brand">
                <a href="#home">EgoGazeVQA</a>
            </div>
            <ul class="nav-menu">
                <li><a href="#abstract">Abstract</a></li>
                <li><a href="#method">Method</a></li>
                <li><a href="#results">Results</a></li>
                <li><a href="#citation">Citation</a></li>
            </ul>
            <div class="hamburger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
    </nav>

    <!-- Hero Section -->
    <section id="home" class="hero">
        <div class="container">
            <h1 class="hero-title">
                <span class="gradient-text">In the Eye of MLLM</span>
            </h1>
            <h2 class="hero-subtitle">
                Benchmarking Egocentric Video Intent Understanding with Gaze-Guided Prompting
            </h2>
            
            <div class="authors">
                <a href="https://taiyi98.github.io" target="_blank">Taiying Peng<sup>1</sup></a>,
                <a href="https://fleurs03.github.io/" target="_blank">Jiacheng Hua<sup>2</sup></a>,
                <a href="https://aptx4869lm.github.io/" target="_blank">Miao Liu<sup>2â€ </sup></a>,
                <a href="https://phi-ai.buaa.edu.cn/" target="_blank">Feng Lu<sup>1â€ </sup></a>
            </div>
            
            <div class="affiliations">
                <sup>1</sup>State Key Laboratory of VR Technology and Systems, School of CSE, Beihang University &nbsp;&nbsp;<br>
                <sup>2</sup>College of AI, Tsinghua University
            </div>
            
            <div class="conference">
                <span class="conference-badge">NeurIPS 2025</span>
            </div>
            
            <div class="hero-buttons">
                <a href="https://arxiv.org/abs/2509.07447" class="btn btn-primary">
                    <i class="fas fa-file-pdf"></i> Paper
                </a>
                <a href="https://github.com/taiyi98/EgoGazeVQA" class="btn btn-secondary">
                    <i class="fab fa-github"></i> Code
                </a>
                <a href="https://huggingface.co/datasets/taiyi09/EgoGazeVQA" class="btn btn-secondary">
                    ðŸ¤— Dataset
                </a>
            </div>
        </div>
    </section>

    <!-- Abstract Section -->
    <section id="abstract" class="section">
        <div class="container">
            <h2 class="section-title">Abstract</h2>
            <div class="abstract-content">
                <p>
                    The emergence of advanced multimodal large language models (MLLMs) has significantly enhanced AI assistants' ability to process complex information across modalities. 
                    Recently, egocentric videos, by directly capturing user focus, actions, and context in an unified coordinate, offer an exciting opportunity to enable proactive 
                    and personalized AI user experiences with MLLMs. However, existing benchmarks overlook the crucial role of gaze as an indicator of user intent.
                </p>
                <p>
                    To address this gap, we introduce <strong>EgoGazeVQA</strong>, an egocentric gaze-guided video question answering benchmark that leverages gaze information 
                    to improve the understanding of longer daily-life videos. EgoGazeVQA consists of 1,757 gaze-based QA pairs from 913 videos, generated 
                    by MLLMs and refined by human annotators. Our experiments reveal that existing MLLMs struggle to accurately interpret user intentions. In contrast, 
                    our <strong>gaze-guided intent prompting methods</strong> significantly enhance performance by integrating spatial, temporal, and intent-related cues.
                </p>
            </div>
            
            <!-- Introduction Figure -->
            <div class="method-overview" style="margin-top: 30px;">
                <img src="assets/images/draw_intro.png" alt="EgoGazeVQA Introduction" class="method-image">
                <p class="image-caption">
                    We propose EgoGazeVQA, the first MLLM benchmark that incorporates essential gaze signals for understanding user intent in egocentric settings. 
                    We present examples of Spatial, Temporal, and Causal Intent QA, demonstrating how gaze information improves MLLMs' performance. 
                    Radar charts compare model performance across different scenarios and activities, showing consistent gains with our gaze-guided prompting strategy.
                </p>
            </div>
        </div>
    </section>

    <!-- Method Section -->
    <section id="method" class="section section-gray">
        <div class="container">
            <h2 class="section-title">Method</h2>
            
            <!-- Pipeline Figure -->
            <div class="method-overview">
                <img src="assets/images/draw_pipeline.png" alt="Construction Pipeline" class="method-image">
                <p class="image-caption">
                    <strong>Construction pipeline of EgoGazeVQA.</strong> We craft the benchmark in three steps: 
                    <strong>Stage 1:</strong> Egocentric video clips are processed to extract frame captions and gaze coordinates to capture user focus. 
                    <strong>Stage 2:</strong> A MLLM model generates spatial/temporal-aware and intention-related Q&A pairs using a customized prompt. 
                    <strong>Stage 3:</strong> Human annotators manually review the generated Q&A pairs for quality dimensions to ensure high-quality data.
                </p>
            </div>
            
            <!-- Prompting Strategies Figure -->
            <div class="method-overview" style="margin-top: 40px;">
                <img src="assets/images/Prompt.png" alt="Gaze-Guided Prompting Strategies" class="method-image">
                <p class="image-caption">
                    <strong>Gaze-guided prompting strategies in EgoGazeVQA.</strong> We experiment with three gaze-guided prompting strategies: 
                    <strong>(Left)</strong> Gaze as Textual Prompt - gaze coordinates are presented as text inputs to guide model responses; 
                    <strong>(Center)</strong> Gaze as Visual Prompt - highlights gaze points directly on video frames; 
                    <strong>(Right)</strong> Gaze Salience Maps as Prompt - utilizes heatmaps of gaze trajectories to provide contextual cues for understanding spatial and temporal intent.
                </p>
            </div>
            
            <div class="method-details" style="margin-top: 40px;">
                <h3>Key Contributions</h3>
                <ul class="feature-list">
                    <li>
                        <i class="fas fa-check-circle"></i>
                        <strong>First Gaze-Guided VQA Benchmark:</strong> EgoGazeVQA is the first egocentric video QA benchmark that leverages gaze information to evaluate MLLM's understanding of human intentions in daily-life videos
                    </li>
                    <li>
                        <i class="fas fa-check-circle"></i>
                        <strong>Three Gaze-Guided Prompting Strategies:</strong> We introduce Gaze as Textual Prompt, Gaze as Visual Prompt, and Sequential Gaze Salience Maps to enhance MLLM's ability to interpret spatial, temporal, and intent-related cues
                    </li>
                    <li>
                        <i class="fas fa-check-circle"></i>
                        <strong>Comprehensive Evaluation:</strong> Extensive experiments on 7 state-of-the-art MLLMs (GPT-4o mini, Gemini 2.0, Qwen2.5-VL, InternVL2.5, etc.) demonstrating significant performance improvements with gaze guidance
                    </li>
                    <li>
                        <i class="fas fa-check-circle"></i>
                        <strong>Fine-tuning Analysis:</strong> Empirical study showing how LoRA fine-tuning enhances model's ability to leverage gaze signals and how gaze estimation accuracy impacts prompting effectiveness
                    </li>
                </ul>
            </div>
        </div>
    </section>

    <!-- Results Section -->
    <section id="results" class="section">
        <div class="container">
            <h2 class="section-title">Experimental Results</h2>
            
            <div class="results-grid">
                <div class="result-card">
                    <h3>Main Results on EgoGazeVQA Benchmark</h3>
                    <div class="table-responsive">
                        <table class="results-table">
                            <thead>
                                <tr>
                                    <th>Method</th>
                                    <th>Strategy</th>
                                    <th>Spatial</th>
                                    <th>Temporal</th>
                                    <th>Causal</th>
                                    <th>Average</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Human</td>
                                    <td>-</td>
                                    <td>80.7</td>
                                    <td>75.6</td>
                                    <td>95.2</td>
                                    <td><strong>83.8</strong></td>
                                </tr>
                                <tr>
                                    <td>Qwen2.5-VL-72B</td>
                                    <td>w/o gaze</td>
                                    <td>57.1</td>
                                    <td>45.2</td>
                                    <td>79.3</td>
                                    <td>60.5</td>
                                </tr>
                                <tr>
                                    <td>Qwen2.5-VL-72B</td>
                                    <td>Textual</td>
                                    <td>60.0</td>
                                    <td>50.7</td>
                                    <td>84.2</td>
                                    <td>65.0</td>
                                </tr>
                                <tr>
                                    <td>Qwen2.5-VL-72B</td>
                                    <td>Visual</td>
                                    <td>59.7</td>
                                    <td>48.1</td>
                                    <td>84.1</td>
                                    <td>63.9</td>
                                </tr>
                                <tr class="highlight">
                                    <td><strong>Qwen2.5-VL-72B</strong></td>
                                    <td><strong>Salience Map</strong></td>
                                    <td><strong>64.3</strong></td>
                                    <td><strong>50.3</strong></td>
                                    <td><strong>84.3</strong></td>
                                    <td><strong>66.3</strong></td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <p style="margin-top: 15px; font-size: 0.95em; color: #666;">
                        <strong>Key Findings:</strong> The best-performing method (Qwen2.5-VL-72B + Salience Map) achieves 66.3% average accuracy, 
                        with <strong>+5.8%</strong> improvement over the baseline. However, there remains a significant gap compared to human performance (83.8%), 
                        highlighting the challenge and room for improvement in gaze-guided egocentric video understanding.
                    </p>
                </div>
            </div>
            
            <!-- Qualitative Results -->
            <div class="visualization-section" style="margin-top: 40px;">
                <h3>Qualitative Results</h3>
                <div class="vis-grid" style="max-width: 100%; padding: 0;">
                    <div class="vis-item" style="grid-column: 1 / -1;">
                        <img src="assets/images/Example_visual.png" alt="Qualitative Examples" style="width: 100%; margin: 0 auto; display: block;">
                        <p style="text-align: justify; margin-top: 15px; max-width: 1200px; margin-left: auto; margin-right: auto;">
                            <strong>Visual examples from our EgoGazeVQA benchmark and gaze-guided prompting.</strong>
                            The successful examples show how gaze signals significantly enhance MLLMs' performance in tasks demanding precise spatial reasoning and intent interpretation, 
                            such as distinguishing closely situated objects in the kitchen and accurately inferring user focus in complex scenes. 
                            Challenging cases reveal limitations when there is drastic body motion or gaze saccades that can mislead the model.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Citation Section -->
    <section id="citation" class="section section-gray">
        <div class="container">
            <h2 class="section-title">Citation</h2>
            
            <div class="citation-box">
                <pre><code class="language-bibtex">@misc{peng2025eyemllmbenchmarkingegocentric,
    title={In the Eye of MLLM: Benchmarking Egocentric Video Intent Understanding with Gaze-Guided Prompting}, 
    author={Taiying Peng and Jiacheng Hua and Miao Liu and Feng Lu},
    year={2025},
    eprint={2509.07447},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2509.07447}
}</code></pre>
                <button class="copy-btn" onclick="copyToClipboard()">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
            
           
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <p>&copy; 2025 EgoGazeVQA Project. All rights reserved.</p>
            </div>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="assets/js/main.js"></script>
    
    <script>
    // Copy to Clipboard Function
    function copyToClipboard() {
        const codeBlock = document.querySelector('.citation-box code');
        const text = codeBlock.textContent;
        
        navigator.clipboard.writeText(text).then(() => {
            const copyBtn = document.querySelector('.copy-btn');
            const originalHTML = copyBtn.innerHTML;
            copyBtn.innerHTML = '<i class="fas fa-check"></i> Copied!';
            copyBtn.style.background = '#10b981';
            
            setTimeout(() => {
                copyBtn.innerHTML = originalHTML;
                copyBtn.style.background = '';
            }, 2000);
        }).catch(err => {
            console.error('Failed to copy text: ', err);
        });
    }
    </script>
</body>
</html>
