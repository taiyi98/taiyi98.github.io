{
  "project": {
    "title": "EgoGazeVQA",
    "subtitle": "Multi-Modal Conversational Balancer for Egocentric Video Understanding",
    "conference": "NeurIPS 2025",
    "status": "Under Review"
  },
  "authors": [
    {
      "name": "Taiyi Pan",
      "affiliation": "1",
      "website": "https://taiyi98.github.io",
      "email": "your.email@example.com"
    },
    {
      "name": "Author Name 2",
      "affiliation": "2",
      "website": "#",
      "email": ""
    },
    {
      "name": "Author Name 3",
      "affiliation": "1",
      "website": "#",
      "email": ""
    }
  ],
  "affiliations": [
    "University Name 1",
    "University Name 2",
    "Research Institute 3"
  ],
  "links": {
    "paper": "https://arxiv.org/abs/your_paper_id",
    "code": "https://github.com/taiyi98/EgoGazeVQA",
    "dataset": "#",
    "video": "https://youtube.com/your_video",
    "poster": "#",
    "slides": "#"
  },
  "abstract": {
    "paragraph1": "Understanding egocentric videos poses unique challenges due to the first-person perspective and dynamic nature of human activities. We present EgoGazeVQA, a novel multi-modal framework that leverages gaze information as an additional modality to enhance video question answering in egocentric scenarios.",
    "paragraph2": "Through extensive experiments on multiple egocentric video datasets, we demonstrate that incorporating gaze patterns significantly improves the model's understanding of user intentions and scene dynamics."
  },
  "results": {
    "main_table": {
      "headers": ["Method", "EgoExo4D", "EGTEA", "Epic-Kitchens"],
      "rows": [
        ["Baseline", "45.2", "38.7", "41.3"],
        ["Method A", "48.5", "42.1", "44.6"],
        ["Method B", "51.3", "44.8", "47.2"],
        ["EgoGazeVQA (Ours)", "58.7", "52.3", "54.1"]
      ]
    },
    "ablation": {
      "labels": ["Full Model", "w/o Gaze", "w/o Temporal", "w/o Balance", "Baseline"],
      "values": [58.7, 52.1, 49.8, 51.3, 45.2]
    }
  },
  "dataset": {
    "stats": {
      "videos": "10K+",
      "qa_pairs": "50K+",
      "gaze_points": "100K+",
      "hours": "500+"
    },
    "features": [
      "High-quality egocentric videos with synchronized gaze tracking",
      "Diverse scenarios including cooking, assembly, and navigation",
      "Multi-level question types: spatial, temporal, and causal reasoning",
      "Professional annotations with quality control"
    ]
  },
  "bibtex": "@inproceedings{pan2025egogaze,\n  title={EgoGazeVQA: Multi-Modal Conversational Balancer for Egocentric Video Understanding},\n  author={Pan, Taiyi and Author2 and Author3},\n  booktitle={Advances in Neural Information Processing Systems},\n  year={2025}\n}",
  "media": {
    "architecture_diagram": "assets/images/architecture.png",
    "result_images": [
      "assets/images/result1.jpg",
      "assets/images/result2.jpg",
      "assets/images/result3.jpg",
      "assets/images/result4.jpg"
    ],
    "demo_video": "assets/videos/demo.mp4",
    "teaser_image": "assets/images/teaser.png"
  },
  "seo": {
    "description": "EgoGazeVQA: Multi-Modal Conversational Balancer for Egocentric Video Understanding - NeurIPS 2025",
    "keywords": "egocentric vision, video understanding, gaze tracking, multimodal learning, NeurIPS 2025, computer vision, deep learning",
    "og_image": "https://taiyi98.github.io/projects/EgoGazeVQA/assets/images/teaser.png"
  },
  "analytics": {
    "google_analytics_id": "UA-XXXXXXXXX-X",
    "enable": false
  }
}
